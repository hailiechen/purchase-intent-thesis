{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b88d00-90e2-401c-b418-ed6fb0b99a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac726f1e-c2ec-4525-8672-87e99afc8e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load dataset and Preprocessing'''\n",
    "df = pd.read_csv(\"online_shoppers_intention.csv\")\n",
    "\n",
    "# remove visitortype is other\n",
    "df = df[df[\"VisitorType\"] != \"Other\"]\n",
    "\n",
    "# transform visitortype, weekend and revenue into numerical\n",
    "df[\"VisitorType\"] = df[\"VisitorType\"].map({\"New_Visitor\": 0, \"Returning_Visitor\": 1})\n",
    "df[\"Weekend\"] = df[\"Weekend\"].map({False: 0, True: 1})\n",
    "df[\"Revenue\"] = df[\"Revenue\"].map({False: 0, True: 1})\n",
    "\n",
    "# transform month into numerical by one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['Month'], prefix='Month')\n",
    "\n",
    "# splitting visitortype into new and returning groups\n",
    "new_visitors = df[df[\"VisitorType\"] == 0]\n",
    "returning_visitors = df[df[\"VisitorType\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c8f5e2-77b3-4458-8de5-2bbc3d177c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get X, y from new / returning visitor, and deleting visitortype\n",
    "X_new = new_visitors.drop(columns=['Revenue', 'VisitorType'])\n",
    "y_new = new_visitors['Revenue']\n",
    "\n",
    "X_return = returning_visitors.drop(columns=['Revenue', 'VisitorType'])\n",
    "y_return = returning_visitors['Revenue']\n",
    "\n",
    "# Columns needed to be standardized\n",
    "numerical_cols = [\n",
    "    'Administrative', 'Administrative_Duration', 'Informational',\n",
    "    'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n",
    "    'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay']\n",
    "\n",
    "# Train-test split\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "    X_new, y_new, test_size=0.2, stratify=y_new, random_state=123)\n",
    "X_train_return, X_test_return, y_train_return, y_test_return = train_test_split(\n",
    "    X_return, y_return, test_size=0.2, stratify=y_return, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a8b883b-31b6-4192-a52b-c8e26bd7e5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params for New Visitors: {'C': 1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best F1-Score for New Visitors: 0.7701433666255743\n",
      "\n",
      "Logistic Regression - New Visitors Results\n",
      "       C penalty     solver  accuracy  precision    recall  f1_score   roc_auc\n",
      "13   1.0      l1  liblinear  0.900369   0.910131  0.668657  0.770143  0.907095\n",
      "12   1.0      l1       saga  0.900369   0.910131  0.668657  0.770143  0.907181\n",
      "19  10.0      l2  liblinear  0.898155   0.906108  0.662774  0.764711  0.903744\n",
      "17  10.0      l1  liblinear  0.898155   0.906108  0.662774  0.764711  0.904123\n",
      "16  10.0      l1       saga  0.898155   0.906108  0.662774  0.764711  0.904313\n",
      "\n",
      "Best Params for Returning Visitors: {'C': 10, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best F1-Score for Returning Visitors: 0.4425239793457375\n",
      "\n",
      "Logistic Regression - Returning Visitors Results\n",
      "       C penalty     solver  accuracy  precision    recall  f1_score   roc_auc\n",
      "17  10.0      l1  liblinear  0.886256   0.698155  0.323963  0.442524  0.883485\n",
      "16  10.0      l1       saga  0.886256   0.698155  0.323963  0.442524  0.883559\n",
      "19  10.0      l2  liblinear  0.886137   0.696771  0.323963  0.442258  0.883539\n",
      "18  10.0      l2       saga  0.886137   0.696771  0.323963  0.442258  0.883574\n",
      "12   1.0      l1       saga  0.886256   0.698896  0.323112  0.441900  0.884230\n"
     ]
    }
   ],
   "source": [
    "'''Step 1 Logistic Regression'''\n",
    "# tuning hyperparameters\n",
    "\n",
    "def cross_validate_lr(X, y, lr_param_grid, label=''):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "\n",
    "    for C in lr_param_grid['C']:\n",
    "        for penalty in lr_param_grid['penalty']:\n",
    "            for solver in lr_param_grid['solver']:\n",
    "                \n",
    "                acc_scores, prec_scores, rec_scores, f1_scores, roc_auc_scores = [], [], [], [], []\n",
    "                \n",
    "                for train_idx, val_idx in skf.split(X, y):\n",
    "                    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                    \n",
    "                    scaler = StandardScaler()\n",
    "                    X_train_scaled = X_train.copy()\n",
    "                    X_val_scaled = X_val.copy()\n",
    "                    X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "                    X_val_scaled[numerical_cols] = scaler.transform(X_val[numerical_cols])\n",
    "\n",
    "                    model = LogisticRegression(C=C, penalty=penalty, solver=solver, max_iter=5000, \n",
    "                                               random_state=123)\n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    y_pred = model.predict(X_val_scaled)\n",
    "                    y_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "                    \n",
    "                    acc_scores.append(accuracy_score(y_val, y_pred))\n",
    "                    prec_scores.append(precision_score(y_val, y_pred, zero_division=0))\n",
    "                    rec_scores.append(recall_score(y_val, y_pred, zero_division=0))\n",
    "                    f1_scores.append(f1_score(y_val, y_pred, zero_division=0))\n",
    "                    roc_auc_scores.append(roc_auc_score(y_val, y_proba))\n",
    "\n",
    "                avg_acc = np.mean(acc_scores)\n",
    "                avg_prec = np.mean(prec_scores)\n",
    "                avg_rec = np.mean(rec_scores)\n",
    "                avg_f1 = np.mean(f1_scores)\n",
    "                avg_roc_auc = np.mean(roc_auc_scores)\n",
    "\n",
    "                results.append({\n",
    "                    'C': C,\n",
    "                    'penalty': penalty,\n",
    "                    'solver': solver,\n",
    "                    'accuracy': avg_acc,\n",
    "                    'precision': avg_prec,\n",
    "                    'recall': avg_rec,\n",
    "                    'f1_score': avg_f1,\n",
    "                    'roc_auc': avg_roc_auc})\n",
    "\n",
    "                if avg_f1 > best_score:\n",
    "                    best_score = avg_f1\n",
    "                    best_params = {\n",
    "                        'C': C,\n",
    "                        'penalty': penalty,\n",
    "                        'solver': solver}\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values(by='f1_score', ascending=False)\n",
    "    print(f\"\\nBest Params for {label}:\", best_params)\n",
    "    print(f\"Best F1-Score for {label}:\", best_score)\n",
    "    print(f\"\\nLogistic Regression - {label} Results\")\n",
    "    print((df_results).head(5))\n",
    "\n",
    "    return best_params, df_results\n",
    "\n",
    "lr_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10], \n",
    "    'penalty': ['l1', 'l2'], \n",
    "    'solver': ['saga', 'liblinear']}\n",
    "lrn_best_param, lrn_results = cross_validate_lr(X_train_new, y_train_new, lr_param_grid, \n",
    "                                                label='New Visitors')\n",
    "lrr_best_param, lrr_results = cross_validate_lr(X_train_return, y_train_return, lr_param_grid, \n",
    "                                                label='Returning Visitors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ccf795a-0374-4b42-aa8f-31155265b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params for New Visitors: {'max_depth': 2, 'criterion': 'entropy'}\n",
      "Best F1-Score for New Visitors: 0.824148307451272\n",
      "\n",
      "Decision Tree - New Visitors Results\n",
      "   max_depth criterion  accuracy  precision    recall  f1_score   roc_auc\n",
      "1          2   entropy  0.918081   0.889797  0.769359  0.824148  0.892593\n",
      "5          4   entropy  0.918081   0.903618  0.754609  0.821500  0.907726\n",
      "4          4      gini  0.917343   0.903520  0.751580  0.819642  0.882961\n",
      "7          5   entropy  0.917343   0.903593  0.751668  0.819506  0.911730\n",
      "3          3   entropy  0.915129   0.893901  0.751624  0.815869  0.903996\n",
      "\n",
      "Best Params for Returning Visitors: {'max_depth': 5, 'criterion': 'entropy'}\n",
      "Best F1-Score for Returning Visitors: 0.5865683928576264\n",
      "\n",
      "Decision Tree - Returning Visitors Results\n",
      "    max_depth criterion  accuracy  precision    recall  f1_score   roc_auc\n",
      "7           5   entropy  0.896801   0.668456  0.525546  0.586568  0.916857\n",
      "6           5      gini  0.897393   0.672065  0.516138  0.583556  0.909834\n",
      "11          7   entropy  0.892299   0.642807  0.515330  0.571090  0.891622\n",
      "5           4   entropy  0.896209   0.685956  0.494089  0.566986  0.916776\n",
      "4           4      gini  0.896445   0.687709  0.492387  0.566612  0.910847\n"
     ]
    }
   ],
   "source": [
    "'''Step 5.2 Decision Tree'''\n",
    "# tuning hyperparameters\n",
    "\n",
    "def cross_validate_dt(X, y, dt_param_grid, label=''):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "\n",
    "    for max_depth in dt_param_grid['max_depth']:\n",
    "        for criterion in dt_param_grid['criterion']:\n",
    "                \n",
    "                acc_scores, prec_scores, rec_scores, f1_scores, roc_auc_scores = [], [], [], [], []\n",
    "                \n",
    "                for train_idx, val_idx in skf.split(X, y):\n",
    "                    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "                    \n",
    "                    model = DecisionTreeClassifier(\n",
    "                    max_depth=max_depth,\n",
    "                    criterion=criterion,\n",
    "                    random_state=123)\n",
    "                \n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_val)\n",
    "                    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "                    \n",
    "                    acc_scores.append(accuracy_score(y_val, y_pred))\n",
    "                    prec_scores.append(precision_score(y_val, y_pred, zero_division=0))\n",
    "                    rec_scores.append(recall_score(y_val, y_pred, zero_division=0))\n",
    "                    f1_scores.append(f1_score(y_val, y_pred, zero_division=0))\n",
    "                    roc_auc_scores.append(roc_auc_score(y_val, y_proba))\n",
    "    \n",
    "                avg_acc = np.mean(acc_scores)\n",
    "                avg_prec = np.mean(prec_scores)\n",
    "                avg_rec = np.mean(rec_scores)\n",
    "                avg_f1 = np.mean(f1_scores)\n",
    "                avg_roc_auc = np.mean(roc_auc_scores)\n",
    "    \n",
    "                results.append({\n",
    "                'max_depth': max_depth,\n",
    "                'criterion': criterion,\n",
    "                'accuracy': avg_acc,\n",
    "                'precision': avg_prec,\n",
    "                'recall': avg_rec,\n",
    "                'f1_score': avg_f1,\n",
    "                'roc_auc': avg_roc_auc})\n",
    "    \n",
    "                if avg_f1 > best_score:\n",
    "                    best_score = avg_f1\n",
    "                    best_params = {\n",
    "                        'max_depth': max_depth,\n",
    "                        'criterion': criterion}\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values(by='f1_score', ascending=False)\n",
    "    print(f\"\\nBest Params for {label}:\", best_params)\n",
    "    print(f\"Best F1-Score for {label}:\", best_score)\n",
    "    print(f\"\\nDecision Tree - {label} Results\")\n",
    "    print((df_results).head(5))\n",
    "\n",
    "    return best_params, df_results\n",
    "\n",
    "\n",
    "dt_param_grid = {\n",
    "        'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        # 'min_samples_leaf': [5, 10, 15],\n",
    "        'criterion': ['gini', 'entropy']}\n",
    "dtn_best_param, dtn_results = cross_validate_dt(X_train_new, y_train_new, dt_param_grid, \n",
    "                                                label='New Visitors')\n",
    "dtr_best_param, dtr_results = cross_validate_dt(X_train_return, y_train_return, dt_param_grid, \n",
    "                                                label='Returning Visitors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea210059-822d-4543-a2ad-6b87c659d151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params for New Visitors: {'criterion': 'gini', 'max_depth': 15, 'n_estimators': 350}\n",
      "Best F1-Score for New Visitors: 0.8284421549463039\n",
      "\n",
      "Random Forest - New Visitors Results\n",
      "max_depth n_estimators criterion  accuracy  precision   recall  f1_score  roc_auc\n",
      "       15          350      gini  0.921771   0.905693 0.765935  0.828442 0.924818\n",
      "       20          300   entropy  0.921033   0.899562 0.768920  0.827715 0.928671\n",
      "       20          400   entropy  0.921033   0.899562 0.768920  0.827715 0.927771\n",
      "       25          350   entropy  0.921033   0.899562 0.768920  0.827715 0.929486\n",
      "       25          400   entropy  0.921033   0.899562 0.768920  0.827715 0.928326\n",
      "Best Params for Returning Visitors: {'criterion': 'entropy', 'max_depth': 15, 'n_estimators': 150}\n",
      "Best F1-Score for Returning Visitors: 0.5898637497752425\n",
      "\n",
      "Random Forest - Returning Visitors Results\n",
      "max_depth n_estimators criterion  accuracy  precision   recall  f1_score  roc_auc\n",
      "       15          150   entropy  0.902488   0.715971 0.502586  0.589864 0.924399\n",
      "       15          250   entropy  0.902607   0.718162 0.500884  0.589292 0.925049\n",
      "       20          200   entropy  0.901303   0.704299 0.506837  0.588877 0.922513\n",
      "       15          300   entropy  0.902251   0.714687 0.501738  0.588758 0.924887\n",
      "       25          150   entropy  0.901422   0.705536 0.505139  0.588185 0.922173\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf = RandomForestClassifier(random_state=123)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [150, 200, 250, 300, 350, 400],\n",
    "    'max_depth': [5, 10, 15, 20, 25],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0),\n",
    "    'f1': make_scorer(f1_score, zero_division=0),\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True)\n",
    "}\n",
    "\n",
    "def tune_and_print(X_train, y_train, label):\n",
    "    gs = GridSearchCV(\n",
    "        rf,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=scoring,\n",
    "        refit='f1',\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    cv = gs.cv_results_\n",
    "    df = pd.DataFrame({\n",
    "        'max_depth': cv['param_max_depth'],\n",
    "        'n_estimators': cv['param_n_estimators'],\n",
    "        'criterion': cv['param_criterion'],\n",
    "        'accuracy': cv['mean_test_accuracy'],\n",
    "        'precision': cv['mean_test_precision'],\n",
    "        'recall': cv['mean_test_recall'],\n",
    "        'f1_score': cv['mean_test_f1'],\n",
    "        'roc_auc': cv['mean_test_roc_auc'],\n",
    "    })\n",
    "\n",
    "    top5 = df.nlargest(5, 'f1_score')\n",
    "\n",
    "    best_p = gs.best_params_\n",
    "    best_f = gs.best_score_\n",
    "\n",
    "    print(f\"Best Params for {label}: {best_p}\")\n",
    "    print(f\"Best F1-Score for {label}: {best_f}\\n\")\n",
    "    print(f\"Random Forest - {label} Results\")\n",
    "    print(top5.to_string(index=False))\n",
    "\n",
    "    return best_p, df\n",
    "\n",
    "rfn_best_param, rfn_results = tune_and_print(X_train_new, y_train_new, 'New Visitors')\n",
    "rfr_best_param, rfr_results = tune_and_print(X_train_return, y_train_return, 'Returning Visitors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5254915c-348d-40c2-bd78-afa9de4a3e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params for New Visitors: {'max_depth': 5, 'n_estimators': 200, 'learning_rate': 0.15}\n",
      "Best F1-Score for New Visitors: 0.8265268826973134\n",
      "\n",
      "Gradient Boosting - New Visitors Results\n",
      "    max_depth  n_estimators  learning_rate  accuracy  precision    recall  \\\n",
      "55          5           200           0.15  0.918081   0.883792  0.778095   \n",
      "51          5           150           0.15  0.917343   0.883653  0.775154   \n",
      "54          5           200           0.10  0.917343   0.884814  0.772256   \n",
      "4           2           200           0.01  0.918081   0.895459  0.763389   \n",
      "8           2           250           0.01  0.918081   0.895459  0.763389   \n",
      "\n",
      "    f1_score   roc_auc  \n",
      "55  0.826527  0.917674  \n",
      "51  0.824630  0.920014  \n",
      "54  0.823702  0.922945  \n",
      "4   0.823195  0.919156  \n",
      "8   0.823195  0.924730  \n",
      "\n",
      "Best Params for Returning Visitors: {'max_depth': 3, 'n_estimators': 300, 'learning_rate': 0.05}\n",
      "Best F1-Score for Returning Visitors: 0.6012272199315454\n",
      "\n",
      "Gradient Boosting - Returning Visitors Results\n",
      "    max_depth  n_estimators  learning_rate  accuracy  precision    recall  \\\n",
      "37          3           300           0.05  0.899408   0.672705  0.544230   \n",
      "26          3           150           0.10  0.899289   0.672336  0.542528   \n",
      "33          3           250           0.05  0.899408   0.676485  0.535719   \n",
      "25          3           150           0.05  0.898934   0.673706  0.535730   \n",
      "44          3           400           0.01  0.900829   0.690704  0.524663   \n",
      "\n",
      "    f1_score   roc_auc  \n",
      "37  0.601227  0.924993  \n",
      "26  0.599993  0.925160  \n",
      "33  0.597436  0.925157  \n",
      "25  0.596326  0.925278  \n",
      "44  0.595847  0.925628  \n"
     ]
    }
   ],
   "source": [
    "'''Step 5.4 Gradient Boosting'''\n",
    "# tuning hyperparameters\n",
    "\n",
    "def cross_validate_gb(X, y, gb_param_grid, label=''):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "\n",
    "    for max_depth in gb_param_grid['max_depth']:\n",
    "        for n_estimators in gb_param_grid['n_estimators']:\n",
    "            for learning_rate in gb_param_grid['learning_rate']:\n",
    "                \n",
    "                acc_scores, prec_scores, rec_scores, f1_scores, roc_auc_scores = [], [], [], [], []\n",
    "                \n",
    "                for train_idx, val_idx in skf.split(X, y):\n",
    "                    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                    \n",
    "                    model = GradientBoostingClassifier(\n",
    "                    max_depth=max_depth,\n",
    "                    n_estimators = n_estimators,\n",
    "                    learning_rate = learning_rate,\n",
    "                    random_state=123)\n",
    "                \n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_val)\n",
    "                    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "                    \n",
    "                    acc_scores.append(accuracy_score(y_val, y_pred))\n",
    "                    prec_scores.append(precision_score(y_val, y_pred, zero_division=0))\n",
    "                    rec_scores.append(recall_score(y_val, y_pred, zero_division=0))\n",
    "                    f1_scores.append(f1_score(y_val, y_pred, zero_division=0))\n",
    "                    roc_auc_scores.append(roc_auc_score(y_val, y_proba))\n",
    "    \n",
    "                avg_acc = np.mean(acc_scores)\n",
    "                avg_prec = np.mean(prec_scores)\n",
    "                avg_rec = np.mean(rec_scores)\n",
    "                avg_f1 = np.mean(f1_scores)\n",
    "                avg_roc_auc = np.mean(roc_auc_scores)\n",
    "    \n",
    "                results.append({\n",
    "                'max_depth': max_depth,\n",
    "                'n_estimators': n_estimators,\n",
    "                'learning_rate': learning_rate,\n",
    "                'accuracy': avg_acc,\n",
    "                'precision': avg_prec,\n",
    "                'recall': avg_rec,\n",
    "                'f1_score': avg_f1,\n",
    "                'roc_auc': avg_roc_auc})\n",
    "    \n",
    "                if avg_f1 > best_score:\n",
    "                    best_score = avg_f1\n",
    "                    best_params = {\n",
    "                        'max_depth': max_depth,\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'learning_rate': learning_rate}\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values(by='f1_score', ascending=False)\n",
    "    print(f\"\\nBest Params for {label}:\", best_params)\n",
    "    print(f\"Best F1-Score for {label}:\", best_score)\n",
    "    print(f\"\\nGradient Boosting - {label} Results\")\n",
    "    print((df_results).head(5))\n",
    "\n",
    "    return best_params, df_results\n",
    "\n",
    "\n",
    "gb_param_grid = {\n",
    "        'max_depth': [2, 3, 5, 10],\n",
    "        'n_estimators': [150,200,250,300, 350, 400], \n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.15]}\n",
    "gbn_best_param, gbn_results = cross_validate_gb(X_train_new, y_train_new, gb_param_grid, \n",
    "                                                label='New Visitors')\n",
    "gbr_best_param, gbr_results = cross_validate_gb(X_train_return, y_train_return, gb_param_grid, \n",
    "                                                label='Returning Visitors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c965b0c4-2f33-4168-9f87-f5ea08f99058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model      Group  Accuracy  Precision    Recall  F1-score  \\\n",
      "0  Logistic Regression        New  0.900369   0.910131  0.668657  0.770143   \n",
      "1        Decision Tree        New  0.918081   0.889797  0.769359  0.824148   \n",
      "2        Random Forest        New  0.921771   0.905693  0.765935  0.828442   \n",
      "3    Gradient Boosting        New  0.918081   0.883792  0.778095  0.826527   \n",
      "4  Logistic Regression  Returning  0.886256   0.698155  0.323963  0.442524   \n",
      "5        Decision Tree  Returning  0.896801   0.668456  0.525546  0.586568   \n",
      "6        Random Forest  Returning  0.902488   0.715971  0.502586  0.589864   \n",
      "7    Gradient Boosting  Returning  0.899408   0.672705  0.544230  0.601227   \n",
      "\n",
      "    ROC AUC  \n",
      "0  0.907095  \n",
      "1  0.892593  \n",
      "2  0.924818  \n",
      "3  0.917674  \n",
      "4  0.883485  \n",
      "5  0.916857  \n",
      "6  0.924399  \n",
      "7  0.924993  \n"
     ]
    }
   ],
   "source": [
    "results_map = {\n",
    "    ('Logistic Regression', 'New'):   lrn_results,\n",
    "    ('Logistic Regression', 'Returning'): lrr_results,\n",
    "    ('Decision Tree', 'New'):         dtn_results,\n",
    "    ('Decision Tree', 'Returning'):   dtr_results,\n",
    "    ('Random Forest', 'New'):         rfn_results,\n",
    "    ('Random Forest', 'Returning'):   rfr_results,\n",
    "    ('Gradient Boosting', 'New'):     gbn_results,\n",
    "    ('Gradient Boosting', 'Returning'): gbr_results,\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for (model, group), df_res in results_map.items():\n",
    "    best = df_res.sort_values('f1_score', ascending=False).iloc[0]\n",
    "    row = {\n",
    "        'Model': model,\n",
    "        'Group': group,\n",
    "        'Accuracy': best['accuracy'],\n",
    "        'Precision': best['precision'],\n",
    "        'Recall': best['recall'],\n",
    "        'F1-score': best['f1_score'],\n",
    "        'ROC AUC': best['roc_auc']\n",
    "    }\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "df_best = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df_best['Model'] = pd.Categorical(df_best['Model'],\n",
    "    categories=['Logistic Regression','Decision Tree','Random Forest','Gradient Boosting'], ordered=True)\n",
    "df_best['Group'] = pd.Categorical(df_best['Group'], categories=['New','Returning'], ordered=True)\n",
    "\n",
    "df_best = df_best.sort_values(['Group','Model']).reset_index(drop=True)\n",
    "\n",
    "print(df_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "153a8839-5471-4676-b1e3-a68cfabc0d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model      Group  accuracy  precision    recall  f1_score  \\\n",
      "0  LogisticRegression        New  0.890855   0.861538  0.666667  0.751678   \n",
      "1        DecisionTree        New  0.911504   0.846154  0.785714  0.814815   \n",
      "2        RandomForest        New  0.911504   0.855263  0.773810  0.812500   \n",
      "3    GradientBoosting        New  0.896755   0.795181  0.785714  0.790419   \n",
      "4  LogisticRegression  Returning  0.882520   0.666667  0.312925  0.425926   \n",
      "5        DecisionTree  Returning  0.897679   0.685714  0.489796  0.571429   \n",
      "6        RandomForest  Returning  0.906679   0.721461  0.537415  0.615984   \n",
      "7    GradientBoosting  Returning  0.909048   0.705645  0.595238  0.645756   \n",
      "\n",
      "    roc_auc  \n",
      "0  0.898553  \n",
      "1  0.894398  \n",
      "2  0.924136  \n",
      "3  0.911345  \n",
      "4  0.896243  \n",
      "5  0.925505  \n",
      "6  0.935612  \n",
      "7  0.937718  \n"
     ]
    }
   ],
   "source": [
    "data_map = {\n",
    "    'New': {\n",
    "        'X_train': X_train_new, 'y_train': y_train_new,\n",
    "        'X_test':  X_test_new,  'y_test':  y_test_new,\n",
    "    },\n",
    "    'Returning': {\n",
    "        'X_train': X_train_return, 'y_train': y_train_return,\n",
    "        'X_test':  X_test_return,  'y_test':  y_test_return,\n",
    "    }\n",
    "}\n",
    "\n",
    "best_params_map = {\n",
    "    'LogisticRegression': {\n",
    "        'New':   lrn_best_param,\n",
    "        'Returning': lrr_best_param,\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'New':   dtn_best_param,\n",
    "        'Returning': dtr_best_param,\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'New':   rfn_best_param,\n",
    "        'Returning': rfr_best_param,\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'New':   gbn_best_param,\n",
    "        'Returning': gbr_best_param,\n",
    "    }\n",
    "}\n",
    "\n",
    "model_map = {\n",
    "    'LogisticRegression': LogisticRegression,\n",
    "    'DecisionTree':        DecisionTreeClassifier,\n",
    "    'RandomForest':        RandomForestClassifier,\n",
    "    'GradientBoosting':    GradientBoostingClassifier,\n",
    "}\n",
    "\n",
    "def make_pipeline(model_name, ModelClass, params):\n",
    "    steps = []\n",
    "    if model_name == 'LogisticRegression':\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "    steps.append(('clf', ModelClass(**params, random_state=123)))\n",
    "    return Pipeline(steps)\n",
    "    \n",
    "def eval_on_test(model_name, X_train, y_train, X_test, y_test, ModelClass, params):\n",
    "    pipe = make_pipeline(model_name, ModelClass, params)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    return {\n",
    "        'accuracy':   accuracy_score(y_test, y_pred),\n",
    "        'precision':  precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall':     recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1_score':   f1_score(y_test, y_pred, zero_division=0),\n",
    "        'roc_auc':    roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "\n",
    "   \n",
    "records = []\n",
    "for model_name, ModelClass in model_map.items():\n",
    "    for group, data in data_map.items():\n",
    "        params = best_params_map[model_name][group]\n",
    "        metrics = eval_on_test(\n",
    "            model_name,\n",
    "            data['X_train'], data['y_train'],\n",
    "            data['X_test'],  data['y_test'],\n",
    "            ModelClass, params\n",
    "        )\n",
    "        row = {'Model': model_name, 'Group': group}\n",
    "        row.update(metrics)\n",
    "        records.append(row)\n",
    "\n",
    "df_eval = pd.DataFrame(records)\n",
    "df_eval['Model'] = pd.Categorical(df_eval['Model'], \n",
    "    categories=['LogisticRegression','DecisionTree','RandomForest','GradientBoosting'], ordered=True)\n",
    "df_eval['Group'] = pd.Categorical(df_eval['Group'], categories=['New','Returning'], ordered=True)\n",
    "df_eval = df_eval.sort_values(['Group','Model']).reset_index(drop=True)\n",
    "\n",
    "print(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d981d-f481-4b41-b397-e68021ccf3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
