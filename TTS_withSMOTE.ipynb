{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff10303c-dc40-4924-a480-db21c52da1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import shap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decf6bd0-0f48-418d-a1a1-3c79ae48009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load dataset and Preprocessing'''\n",
    "df = pd.read_csv(\"online_shoppers_intention.csv\")\n",
    "\n",
    "# remove visitortype is other\n",
    "df = df[df[\"VisitorType\"] != \"Other\"]\n",
    "\n",
    "# transform visitortype, weekend and revenue into numerical\n",
    "df[\"VisitorType\"] = df[\"VisitorType\"].map({\"New_Visitor\": 0, \"Returning_Visitor\": 1})\n",
    "df[\"Weekend\"] = df[\"Weekend\"].map({False: 0, True: 1})\n",
    "df[\"Revenue\"] = df[\"Revenue\"].map({False: 0, True: 1})\n",
    "\n",
    "# transform month into numerical by one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['Month'], prefix='Month')\n",
    "\n",
    "# splitting visitortype into new and returning groups\n",
    "new_visitors = df[df[\"VisitorType\"] == 0]\n",
    "returning_visitors = df[df[\"VisitorType\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88bdcbac-c3b6-46bd-9ff1-5cbbffd031ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get X, y from new / returning visitor, and deleting visitortype\n",
    "X_new = new_visitors.drop(columns=['Revenue', 'VisitorType'])\n",
    "y_new = new_visitors['Revenue']\n",
    "\n",
    "X_return = returning_visitors.drop(columns=['Revenue', 'VisitorType'])\n",
    "y_return = returning_visitors['Revenue']\n",
    "\n",
    "# Columns needed to be standardized\n",
    "numerical_cols = [\n",
    "    'Administrative', 'Administrative_Duration', 'Informational',\n",
    "    'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',\n",
    "    'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay']\n",
    "\n",
    "# train-test split for new/ returning visitors\n",
    "X_train_newf, X_test_new, y_train_newf, y_test_new = train_test_split(\n",
    "    X_new, y_new, test_size=0.2, random_state=123, stratify=y_new)\n",
    "\n",
    "X_train_returnf, X_test_return, y_train_returnf, y_test_return = train_test_split(\n",
    "    X_return, y_return, test_size=0.2, random_state=123, stratify=y_return)\n",
    "\n",
    "# validation split for tuning parameters\n",
    "X_train_new, X_val_new, y_train_new, y_val_new = train_test_split(\n",
    "    X_train_newf, y_train_newf, test_size=0.2, random_state=123, stratify=y_train_newf)\n",
    "\n",
    "X_train_return, X_val_return, y_train_return, y_val_return = train_test_split(\n",
    "    X_train_returnf, y_train_returnf, test_size=0.2, random_state=123, stratify=y_train_returnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d436d8-42ed-4e99-81ef-cce3c1d5bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_new_scaled = X_train_new.copy()\n",
    "X_val_new_scaled = X_val_new.copy()\n",
    "X_test_new_scaled = X_test_new.copy()\n",
    "\n",
    "X_train_return_scaled = X_train_return.copy()\n",
    "X_val_return_scaled = X_val_return.copy()\n",
    "X_test_return_scaled = X_test_return.copy()\n",
    "\n",
    "X_train_new_scaled[numerical_cols] = scaler.fit_transform(X_train_new[numerical_cols])\n",
    "X_val_new_scaled[numerical_cols] = scaler.transform(X_val_new[numerical_cols])\n",
    "X_test_new_scaled[numerical_cols] = scaler.transform(X_test_new[numerical_cols])\n",
    "\n",
    "X_train_return_scaled[numerical_cols] = scaler.fit_transform(X_train_return[numerical_cols])\n",
    "X_val_return_scaled[numerical_cols] = scaler.transform(X_val_return[numerical_cols])\n",
    "X_test_return_scaled[numerical_cols] = scaler.transform(X_test_return[numerical_cols])\n",
    "\n",
    "smote = SMOTE(random_state=123)\n",
    "X_train_new_resampled, y_train_new_resampled = smote.fit_resample(X_train_new_scaled, y_train_new)\n",
    "X_train_return_resampled, y_train_return_resampled = smote.fit_resample(X_train_return_scaled, y_train_return)\n",
    "\n",
    "smote = SMOTE(random_state=123)\n",
    "X_train_new_resampled_unscaled, y_train_new_resampled_unscaled = smote.fit_resample(X_train_new, y_train_new)\n",
    "X_train_return_resampled_unscaled, y_train_return_resampled_unscaled = smote.fit_resample(\n",
    "    X_train_return, y_train_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "891b7b30-df86-411b-a3b3-2e6c3404ba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params for New Visitors: {'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best F1-Score for New Visitors: 0.8208955223880597\n",
      "\n",
      "Logistic Regression - New Visitors Results\n",
      "         C penalty     solver  accuracy  precision    recall  f1_score  \\\n",
      "8    0.100      l1       saga  0.911439   0.833333  0.808824  0.820896   \n",
      "9    0.100      l1  liblinear  0.911439   0.833333  0.808824  0.820896   \n",
      "12   1.000      l1       saga  0.904059   0.828125  0.779412  0.803030   \n",
      "15   1.000      l2  liblinear  0.904059   0.828125  0.779412  0.803030   \n",
      "14   1.000      l2       saga  0.904059   0.828125  0.779412  0.803030   \n",
      "5    0.010      l1  liblinear  0.904059   0.828125  0.779412  0.803030   \n",
      "13   1.000      l1  liblinear  0.904059   0.828125  0.779412  0.803030   \n",
      "6    0.010      l2       saga  0.904059   0.838710  0.764706  0.800000   \n",
      "7    0.010      l2  liblinear  0.904059   0.838710  0.764706  0.800000   \n",
      "10   0.100      l2       saga  0.900369   0.825397  0.764706  0.793893   \n",
      "18  10.000      l2       saga  0.900369   0.825397  0.764706  0.793893   \n",
      "19  10.000      l2  liblinear  0.900369   0.825397  0.764706  0.793893   \n",
      "11   0.100      l2  liblinear  0.896679   0.812500  0.764706  0.787879   \n",
      "4    0.010      l1       saga  0.896679   0.822581  0.750000  0.784615   \n",
      "16  10.000      l1       saga  0.896679   0.822581  0.750000  0.784615   \n",
      "17  10.000      l1  liblinear  0.896679   0.822581  0.750000  0.784615   \n",
      "2    0.001      l2       saga  0.885609   0.776119  0.764706  0.770370   \n",
      "3    0.001      l2  liblinear  0.881919   0.772727  0.750000  0.761194   \n",
      "0    0.001      l1       saga  0.250923   0.250923  1.000000  0.401180   \n",
      "1    0.001      l1  liblinear  0.749077   0.000000  0.000000  0.000000   \n",
      "\n",
      "     roc_auc  \n",
      "8   0.923428  \n",
      "9   0.923066  \n",
      "12  0.911547  \n",
      "15  0.911258  \n",
      "14  0.913576  \n",
      "5   0.866524  \n",
      "13  0.910895  \n",
      "6   0.899160  \n",
      "7   0.899449  \n",
      "10  0.911910  \n",
      "18  0.911692  \n",
      "19  0.911547  \n",
      "11  0.910895  \n",
      "4   0.896950  \n",
      "16  0.911113  \n",
      "17  0.910968  \n",
      "2   0.869313  \n",
      "3   0.875543  \n",
      "0   0.500000  \n",
      "1   0.500000  \n",
      "\n",
      "Best Params for Returning Visitors: {'C': 0.001, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best F1-Score for Returning Visitors: 0.6057142857142858\n",
      "\n",
      "Logistic Regression - Returning Visitors Results\n",
      "         C penalty     solver  accuracy  precision    recall  f1_score  \\\n",
      "1    0.001      l1  liblinear  0.877370   0.548276  0.676596  0.605714   \n",
      "0    0.001      l1       saga  0.883294   0.578512  0.595745  0.587002   \n",
      "6    0.010      l2       saga  0.861374   0.501511  0.706383  0.586572   \n",
      "7    0.010      l2  liblinear  0.854265   0.483965  0.706383  0.574394   \n",
      "10   0.100      l2       saga  0.863744   0.508197  0.659574  0.574074   \n",
      "11   0.100      l2  liblinear  0.861967   0.503268  0.655319  0.569316   \n",
      "3    0.001      l2  liblinear  0.847749   0.469274  0.714894  0.566610   \n",
      "17  10.000      l1  liblinear  0.864336   0.510345  0.629787  0.563810   \n",
      "9    0.100      l1  liblinear  0.861374   0.501661  0.642553  0.563433   \n",
      "4    0.010      l1       saga  0.836493   0.448363  0.757447  0.563291   \n",
      "13   1.000      l1  liblinear  0.863744   0.508591  0.629787  0.562738   \n",
      "16  10.000      l1       saga  0.863744   0.508591  0.629787  0.562738   \n",
      "18  10.000      l2       saga  0.863744   0.508591  0.629787  0.562738   \n",
      "19  10.000      l2  liblinear  0.863744   0.508591  0.629787  0.562738   \n",
      "2    0.001      l2       saga  0.849526   0.472464  0.693617  0.562069   \n",
      "8    0.100      l1       saga  0.861374   0.501672  0.638298  0.561798   \n",
      "12   1.000      l1       saga  0.863152   0.506849  0.629787  0.561670   \n",
      "14   1.000      l2       saga  0.863152   0.506849  0.629787  0.561670   \n",
      "15   1.000      l2  liblinear  0.863152   0.506849  0.629787  0.561670   \n",
      "5    0.010      l1  liblinear  0.838270   0.450262  0.731915  0.557536   \n",
      "\n",
      "     roc_auc  \n",
      "1   0.877835  \n",
      "0   0.883897  \n",
      "6   0.886893  \n",
      "7   0.884934  \n",
      "10  0.882178  \n",
      "11  0.881665  \n",
      "3   0.882356  \n",
      "17  0.875703  \n",
      "9   0.879094  \n",
      "4   0.890232  \n",
      "13  0.876197  \n",
      "16  0.875864  \n",
      "18  0.876110  \n",
      "19  0.876028  \n",
      "2   0.884137  \n",
      "8   0.878968  \n",
      "12  0.876095  \n",
      "14  0.877694  \n",
      "15  0.877832  \n",
      "5   0.887520  \n"
     ]
    }
   ],
   "source": [
    "'''Step 1 Logistic Regression'''\n",
    "# tuning hyperparameters\n",
    "\n",
    "def tune_tts_lr(X_train, y_train, X_val, y_val, label=''):\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10], \n",
    "        'penalty': ['l1', 'l2'], \n",
    "        'solver': ['saga', 'liblinear']}\n",
    "\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "\n",
    "    for C in param_grid['C']:\n",
    "        for penalty in param_grid['penalty']:\n",
    "            for solver in param_grid['solver']:\n",
    "                model = LogisticRegression(\n",
    "                    C=C,\n",
    "                    penalty=penalty,\n",
    "                    solver=solver,\n",
    "                    max_iter=5000,\n",
    "                    random_state=123)\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "                acc = accuracy_score(y_val, y_pred)\n",
    "                prec = precision_score(y_val, y_pred, zero_division=0)\n",
    "                rec = recall_score(y_val, y_pred, zero_division=0)\n",
    "                f1 = f1_score(y_val, y_pred, zero_division=0)\n",
    "                roc_auc = roc_auc_score(y_val, y_proba)\n",
    "    \n",
    "                results.append({\n",
    "                    'C': C,\n",
    "                    'penalty': penalty,\n",
    "                    'solver': solver,\n",
    "                    'accuracy': acc,\n",
    "                    'precision': prec,\n",
    "                    'recall': rec,\n",
    "                    'f1_score': f1,\n",
    "                    'roc_auc': roc_auc})\n",
    "    \n",
    "                if f1 > best_score:\n",
    "                    best_score = f1\n",
    "                    best_params = {\n",
    "                        'C': C,\n",
    "                        'penalty': penalty,\n",
    "                        'solver': solver}\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values(by='f1_score', ascending=False)\n",
    "    print(f\"\\nBest Params for {label}:\", best_params)\n",
    "    print(f\"Best F1-Score for {label}:\", best_score)\n",
    "    print(f\"\\nLogistic Regression - {label} Results\")\n",
    "    print(df_results)\n",
    "    return best_params, df_results\n",
    "\n",
    "\n",
    "\n",
    "lrn_best_param, lrn_results = tune_tts_lr(\n",
    "    X_train_new_resampled,\n",
    "    y_train_new_resampled,\n",
    "    X_val_new_scaled,\n",
    "    y_val_new,\n",
    "    label='New Visitors')\n",
    "\n",
    "lrr_best_param, lrr_results = tune_tts_lr(\n",
    "    X_train_return_resampled,\n",
    "    y_train_return_resampled,\n",
    "    X_val_return_scaled,\n",
    "    y_val_return,\n",
    "    label='Returning Visitors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8dcd74f-5493-43b3-8380-7a24230a5834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params for New Visitors: {'max_depth': 2, 'criterion': 'gini'}\n",
      "Best F1-Score for New Visitors: 0.8321167883211679\n",
      "\n",
      "Decision Tree - New Visitors Results\n",
      "    max_depth criterion  accuracy  precision    recall  f1_score   roc_auc\n",
      "0           2      gini  0.915129   0.826087  0.838235  0.832117  0.904376\n",
      "1           2   entropy  0.915129   0.826087  0.838235  0.832117  0.905571\n",
      "13          8   entropy  0.911439   0.805556  0.852941  0.828571  0.917560\n",
      "11          7   entropy  0.911439   0.805556  0.852941  0.828571  0.915278\n",
      "5           4   entropy  0.911439   0.814286  0.838235  0.826087  0.939474\n",
      "9           6   entropy  0.911439   0.814286  0.838235  0.826087  0.915858\n",
      "7           5   entropy  0.900369   0.773333  0.852941  0.811189  0.939221\n",
      "4           4      gini  0.900369   0.780822  0.838235  0.808511  0.894342\n",
      "15          9   entropy  0.896679   0.763158  0.852941  0.805556  0.913141\n",
      "17         10   entropy  0.896679   0.763158  0.852941  0.805556  0.907925\n",
      "8           6      gini  0.896679   0.777778  0.823529  0.800000  0.869205\n",
      "2           3      gini  0.892989   0.753247  0.852941  0.800000  0.924696\n",
      "3           3   entropy  0.889299   0.737500  0.867647  0.797297  0.932375\n",
      "6           5      gini  0.892989   0.767123  0.823529  0.794326  0.892205\n",
      "10          7      gini  0.892989   0.782609  0.794118  0.788321  0.850804\n",
      "12          8      gini  0.885609   0.753425  0.808824  0.780142  0.841495\n",
      "14          9      gini  0.874539   0.717949  0.823529  0.767123  0.860077\n",
      "16         10      gini  0.859779   0.682927  0.823529  0.746667  0.827333\n",
      "\n",
      "Best Params for Returning Visitors: {'max_depth': 8, 'criterion': 'entropy'}\n",
      "Best F1-Score for Returning Visitors: 0.6286836935166994\n",
      "\n",
      "Decision Tree - Returning Visitors Results\n",
      "    max_depth criterion  accuracy  precision    recall  f1_score   roc_auc\n",
      "13          8   entropy  0.888033   0.583942  0.680851  0.628684  0.881122\n",
      "11          7   entropy  0.888033   0.584559  0.676596  0.627219  0.890060\n",
      "9           6   entropy  0.880924   0.556291  0.714894  0.625698  0.901672\n",
      "7           5   entropy  0.877962   0.546926  0.719149  0.621324  0.901819\n",
      "1           2   entropy  0.866114   0.512535  0.782979  0.619529  0.885367\n",
      "0           2      gini  0.866114   0.512535  0.782979  0.619529  0.885367\n",
      "3           3   entropy  0.866114   0.512535  0.782979  0.619529  0.892716\n",
      "5           4   entropy  0.876777   0.543408  0.719149  0.619048  0.897886\n",
      "2           3      gini  0.866706   0.514205  0.770213  0.616695  0.893105\n",
      "6           5      gini  0.879739   0.554422  0.693617  0.616257  0.891278\n",
      "4           4      gini  0.877370   0.546053  0.706383  0.615955  0.896185\n",
      "14          9      gini  0.885071   0.577358  0.651064  0.612000  0.848307\n",
      "8           6      gini  0.880332   0.557895  0.676596  0.611538  0.889747\n",
      "10          7      gini  0.879739   0.557554  0.659574  0.604288  0.875130\n",
      "15          9   entropy  0.883886   0.576471  0.625532  0.600000  0.869236\n",
      "12          8      gini  0.884479   0.580000  0.617021  0.597938  0.865115\n",
      "16         10      gini  0.880332   0.564202  0.617021  0.589431  0.823294\n",
      "17         10   entropy  0.881517   0.571429  0.595745  0.583333  0.831211\n"
     ]
    }
   ],
   "source": [
    "'''Step 2 Decision Tree'''\n",
    "# tuning hyperparameters\n",
    "\n",
    "def tune_decision_tree(X_train, y_train, X_val, y_val, label=''):\n",
    "    param_grid = {\n",
    "        'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'criterion': ['gini', 'entropy']}\n",
    "\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for criterion in param_grid['criterion']:\n",
    "            model = DecisionTreeClassifier(\n",
    "                max_depth=max_depth,\n",
    "                criterion=criterion,\n",
    "                random_state=123)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            prec = precision_score(y_val, y_pred)\n",
    "            rec = recall_score(y_val, y_pred)\n",
    "            f1 = f1_score(y_val, y_pred)\n",
    "            roc_auc = roc_auc_score(y_val, y_proba)\n",
    "\n",
    "            results.append({\n",
    "                'max_depth': max_depth,\n",
    "                'criterion': criterion,\n",
    "                'accuracy': acc,\n",
    "                'precision': prec,\n",
    "                'recall': rec,\n",
    "                'f1_score': f1,\n",
    "                'roc_auc': roc_auc})\n",
    "\n",
    "            if f1 > best_score:\n",
    "                best_score = f1\n",
    "                best_params = {\n",
    "                    'max_depth': max_depth,\n",
    "                    'criterion': criterion}\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values(by='f1_score', ascending=False)\n",
    "    print(f\"\\nBest Params for {label}:\", best_params)\n",
    "    print(f\"Best F1-Score for {label}:\", best_score)\n",
    "    print(f\"\\nDecision Tree - {label} Results\")\n",
    "    print(df_results)\n",
    "\n",
    "    return best_params, df_results\n",
    "\n",
    "dtn_best_param, dtn_results = tune_decision_tree(\n",
    "    X_train_new_resampled_unscaled,\n",
    "    y_train_new_resampled_unscaled,\n",
    "    X_val_new,\n",
    "    y_val_new,\n",
    "    label='New Visitors')\n",
    "\n",
    "dtr_best_param, dtr_results = tune_decision_tree(\n",
    "    X_train_return_resampled_unscaled,\n",
    "    y_train_return_resampled_unscaled,\n",
    "    X_val_return,\n",
    "    y_val_return,\n",
    "    label='Returning Visitors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773168f2-1fd5-450f-a0e6-8504adcfafda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params for New Visitors: {'n_estimators': 150, 'max_depth': 5, 'criterion': 'gini'}\n",
      "Best F1-Score for New Visitors: 0.8405797101449275\n",
      "\n",
      "Random Forest - New Visitors Results\n",
      "    n_estimators  max_depth  accuracy  precision    recall  f1_score   roc_auc\n",
      "0            150          5  0.918819   0.828571  0.852941  0.840580  0.947117\n",
      "32           300         10  0.918819   0.828571  0.852941  0.840580  0.946103\n",
      "22           250         10  0.918819   0.828571  0.852941  0.840580  0.945016\n",
      "23           250         10  0.918819   0.828571  0.852941  0.840580  0.945016\n",
      "24           250         15  0.918819   0.828571  0.852941  0.840580  0.937337\n",
      "25           250         15  0.918819   0.828571  0.852941  0.840580  0.937337\n",
      "1            150          5  0.918819   0.828571  0.852941  0.840580  0.947117\n",
      "31           300          5  0.918819   0.828571  0.852941  0.840580  0.945161\n",
      "33           300         10  0.918819   0.828571  0.852941  0.840580  0.946103\n",
      "20           250          5  0.918819   0.828571  0.852941  0.840580  0.946320\n",
      "34           300         15  0.918819   0.828571  0.852941  0.840580  0.937120\n",
      "35           300         15  0.918819   0.828571  0.852941  0.840580  0.937120\n",
      "40           350          5  0.918819   0.828571  0.852941  0.840580  0.945813\n",
      "41           350          5  0.918819   0.828571  0.852941  0.840580  0.945813\n",
      "50           400          5  0.918819   0.828571  0.852941  0.840580  0.946103\n",
      "51           400          5  0.918819   0.828571  0.852941  0.840580  0.946103\n",
      "21           250          5  0.918819   0.828571  0.852941  0.840580  0.946320\n",
      "30           300          5  0.918819   0.828571  0.852941  0.840580  0.945161\n",
      "10           200          5  0.918819   0.828571  0.852941  0.840580  0.947334\n",
      "11           200          5  0.918819   0.828571  0.852941  0.840580  0.947334\n",
      "13           200         10  0.918819   0.828571  0.852941  0.840580  0.946827\n",
      "12           200         10  0.918819   0.828571  0.852941  0.840580  0.946827\n",
      "4            150         15  0.915129   0.816901  0.852941  0.834532  0.937663\n",
      "5            150         15  0.915129   0.816901  0.852941  0.834532  0.937663\n",
      "45           350         15  0.915129   0.816901  0.852941  0.834532  0.935961\n",
      "3            150         10  0.915129   0.816901  0.852941  0.834532  0.947624\n",
      "2            150         10  0.915129   0.816901  0.852941  0.834532  0.947624\n",
      "44           350         15  0.915129   0.816901  0.852941  0.834532  0.935961\n",
      "42           350         10  0.915129   0.826087  0.838235  0.832117  0.947262\n",
      "52           400         10  0.915129   0.826087  0.838235  0.832117  0.946610\n",
      "43           350         10  0.915129   0.826087  0.838235  0.832117  0.947262\n",
      "53           400         10  0.915129   0.826087  0.838235  0.832117  0.946610\n",
      "14           200         15  0.911439   0.805556  0.852941  0.828571  0.937917\n",
      "15           200         15  0.911439   0.805556  0.852941  0.828571  0.937917\n",
      "54           400         15  0.911439   0.814286  0.838235  0.826087  0.938025\n",
      "55           400         15  0.911439   0.814286  0.838235  0.826087  0.938025\n",
      "56           400         20  0.911439   0.814286  0.838235  0.826087  0.937735\n",
      "49           350         25  0.911439   0.814286  0.838235  0.826087  0.939003\n",
      "48           350         25  0.911439   0.814286  0.838235  0.826087  0.939003\n",
      "47           350         20  0.911439   0.814286  0.838235  0.826087  0.937880\n",
      "46           350         20  0.911439   0.814286  0.838235  0.826087  0.937880\n",
      "57           400         20  0.911439   0.814286  0.838235  0.826087  0.937735\n",
      "58           400         25  0.911439   0.814286  0.838235  0.826087  0.939221\n",
      "18           200         25  0.911439   0.814286  0.838235  0.826087  0.936576\n",
      "39           300         25  0.911439   0.814286  0.838235  0.826087  0.939221\n",
      "19           200         25  0.911439   0.814286  0.838235  0.826087  0.936576\n",
      "38           300         25  0.911439   0.814286  0.838235  0.826087  0.939221\n",
      "6            150         20  0.911439   0.814286  0.838235  0.826087  0.936359\n",
      "7            150         20  0.911439   0.814286  0.838235  0.826087  0.936359\n",
      "8            150         25  0.911439   0.814286  0.838235  0.826087  0.937663\n",
      "9            150         25  0.911439   0.814286  0.838235  0.826087  0.937663\n",
      "16           200         20  0.911439   0.814286  0.838235  0.826087  0.936395\n",
      "17           200         20  0.911439   0.814286  0.838235  0.826087  0.936395\n",
      "59           400         25  0.911439   0.814286  0.838235  0.826087  0.939221\n",
      "37           300         20  0.907749   0.802817  0.838235  0.820144  0.938387\n",
      "36           300         20  0.907749   0.802817  0.838235  0.820144  0.938387\n",
      "29           250         25  0.907749   0.802817  0.838235  0.820144  0.938931\n",
      "28           250         25  0.907749   0.802817  0.838235  0.820144  0.938931\n",
      "27           250         20  0.907749   0.802817  0.838235  0.820144  0.938025\n",
      "26           250         20  0.907749   0.802817  0.838235  0.820144  0.938025\n",
      "\n",
      "Best Params for Returning Visitors: {'n_estimators': 150, 'max_depth': 10, 'criterion': 'gini'}\n",
      "Best F1-Score for Returning Visitors: 0.6370370370370371\n",
      "\n",
      "Random Forest - Returning Visitors Results\n",
      "    n_estimators  max_depth  accuracy  precision    recall  f1_score   roc_auc\n",
      "2            150         10  0.883886   0.563934  0.731915  0.637037  0.921884\n",
      "3            150         10  0.883886   0.563934  0.731915  0.637037  0.921884\n",
      "12           200         10  0.883294   0.562500  0.727660  0.634508  0.922795\n",
      "13           200         10  0.883294   0.562500  0.727660  0.634508  0.922795\n",
      "42           350         10  0.882109   0.558824  0.727660  0.632163  0.923097\n",
      "43           350         10  0.882109   0.558824  0.727660  0.632163  0.923097\n",
      "44           350         15  0.890995   0.596226  0.672340  0.632000  0.922338\n",
      "45           350         15  0.890995   0.596226  0.672340  0.632000  0.922338\n",
      "22           250         10  0.882701   0.561056  0.723404  0.631970  0.922977\n",
      "23           250         10  0.882701   0.561056  0.723404  0.631970  0.922977\n",
      "8            150         25  0.894550   0.615385  0.646809  0.630705  0.921231\n",
      "9            150         25  0.894550   0.615385  0.646809  0.630705  0.921231\n",
      "32           300         10  0.881517   0.557377  0.723404  0.629630  0.923158\n",
      "33           300         10  0.881517   0.557377  0.723404  0.629630  0.923158\n",
      "24           250         15  0.890403   0.594697  0.668085  0.629259  0.922578\n",
      "25           250         15  0.890403   0.594697  0.668085  0.629259  0.922578\n",
      "52           400         10  0.880924   0.555556  0.723404  0.628466  0.922968\n",
      "53           400         10  0.880924   0.555556  0.723404  0.628466  0.922968\n",
      "11           200          5  0.870261   0.522727  0.782979  0.626917  0.911772\n",
      "10           200          5  0.870261   0.522727  0.782979  0.626917  0.911772\n",
      "15           200         15  0.889218   0.590226  0.668085  0.626747  0.922417\n",
      "14           200         15  0.889218   0.590226  0.668085  0.626747  0.922417\n",
      "54           400         15  0.889810   0.593156  0.663830  0.626506  0.922400\n",
      "55           400         15  0.889810   0.593156  0.663830  0.626506  0.922400\n",
      "50           400          5  0.869668   0.521246  0.782979  0.625850  0.909312\n",
      "51           400          5  0.869668   0.521246  0.782979  0.625850  0.909312\n",
      "31           300          5  0.869668   0.521246  0.782979  0.625850  0.911045\n",
      "41           350          5  0.869668   0.521246  0.782979  0.625850  0.910302\n",
      "40           350          5  0.869668   0.521246  0.782979  0.625850  0.910302\n",
      "0            150          5  0.869668   0.521246  0.782979  0.625850  0.909757\n",
      "30           300          5  0.869668   0.521246  0.782979  0.625850  0.911045\n",
      "1            150          5  0.869668   0.521246  0.782979  0.625850  0.909757\n",
      "20           250          5  0.869668   0.521246  0.782979  0.625850  0.910761\n",
      "21           250          5  0.869668   0.521246  0.782979  0.625850  0.910761\n",
      "34           300         15  0.889218   0.590909  0.663830  0.625251  0.922587\n",
      "35           300         15  0.889218   0.590909  0.663830  0.625251  0.922587\n",
      "4            150         15  0.887441   0.584906  0.659574  0.620000  0.922413\n",
      "5            150         15  0.887441   0.584906  0.659574  0.620000  0.922413\n",
      "38           300         25  0.890995   0.604082  0.629787  0.616667  0.921322\n",
      "39           300         25  0.890995   0.604082  0.629787  0.616667  0.921322\n",
      "28           250         25  0.890995   0.604938  0.625532  0.615063  0.921206\n",
      "29           250         25  0.890995   0.604938  0.625532  0.615063  0.921206\n",
      "19           200         25  0.889810   0.599190  0.629787  0.614108  0.921342\n",
      "18           200         25  0.889810   0.599190  0.629787  0.614108  0.921342\n",
      "48           350         25  0.889218   0.596774  0.629787  0.612836  0.920744\n",
      "49           350         25  0.889218   0.596774  0.629787  0.612836  0.920744\n",
      "17           200         20  0.888033   0.591270  0.634043  0.611910  0.920083\n",
      "16           200         20  0.888033   0.591270  0.634043  0.611910  0.920083\n",
      "58           400         25  0.888033   0.592000  0.629787  0.610309  0.921073\n",
      "59           400         25  0.888033   0.592000  0.629787  0.610309  0.921073\n",
      "7            150         20  0.886256   0.584980  0.629787  0.606557  0.920104\n",
      "6            150         20  0.886256   0.584980  0.629787  0.606557  0.920104\n",
      "26           250         20  0.886256   0.585657  0.625532  0.604938  0.920606\n",
      "27           250         20  0.886256   0.585657  0.625532  0.604938  0.920606\n",
      "56           400         20  0.885664   0.584677  0.617021  0.600414  0.920751\n",
      "57           400         20  0.885664   0.584677  0.617021  0.600414  0.920751\n",
      "37           300         20  0.885071   0.582329  0.617021  0.599174  0.921099\n",
      "36           300         20  0.885071   0.582329  0.617021  0.599174  0.921099\n",
      "46           350         20  0.885071   0.582996  0.612766  0.597510  0.920799\n",
      "47           350         20  0.885071   0.582996  0.612766  0.597510  0.920799\n"
     ]
    }
   ],
   "source": [
    "'''Step 3 Random Forest'''\n",
    "# tuning hyperparameters\n",
    "\n",
    "def tune_random_forest(X_train, y_train, X_val, y_val, label=''):\n",
    "    param_grid = {\n",
    "        'max_depth': [5, 10, 15, 20, 25],\n",
    "        'n_estimators': [150, 200, 250, 300, 350, 400],\n",
    "        'criterion': ['gini', 'entropy']}\n",
    "\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "\n",
    "    for n in param_grid['n_estimators']:\n",
    "        for depth in param_grid['max_depth']:\n",
    "            for criterion in param_grid['criterion']:\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=n,\n",
    "                    max_depth=depth,\n",
    "                    random_state=123)\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "                acc = accuracy_score(y_val, y_pred)\n",
    "                prec = precision_score(y_val, y_pred)\n",
    "                rec = recall_score(y_val, y_pred)\n",
    "                f1 = f1_score(y_val, y_pred)\n",
    "                roc_auc = roc_auc_score(y_val, y_proba)\n",
    "    \n",
    "                results.append({\n",
    "                    'n_estimators': n,\n",
    "                    'max_depth': depth,\n",
    "                    'accuracy': acc,\n",
    "                    'precision': prec,\n",
    "                    'recall': rec,\n",
    "                    'f1_score': f1,\n",
    "                    'roc_auc': roc_auc})\n",
    "    \n",
    "                if f1 > best_score:\n",
    "                    best_score = f1\n",
    "                    best_params = {\n",
    "                        'n_estimators': n,\n",
    "                        'max_depth': depth,\n",
    "                        'criterion': criterion}\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values(by='f1_score', ascending=False)\n",
    "    print(f\"\\nBest Params for {label}:\", best_params)\n",
    "    print(f\"Best F1-Score for {label}:\", best_score)\n",
    "    print(f\"\\nRandom Forest - {label} Results\")\n",
    "    print(df_results)\n",
    "\n",
    "    return best_params, df_results\n",
    "\n",
    "\n",
    "rfn_best_param, rfn_results = tune_random_forest(\n",
    "    X_train_new_resampled_unscaled,\n",
    "    y_train_new_resampled_unscaled,\n",
    "    X_val_new,\n",
    "    y_val_new,\n",
    "    label='New Visitors')\n",
    "\n",
    "rfr_best_param, rfr_results = tune_random_forest(\n",
    "    X_train_return_resampled_unscaled,\n",
    "    y_train_return_resampled_unscaled,\n",
    "    X_val_return,\n",
    "    y_val_return,\n",
    "    label='Returning Visitors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c695223-3e0e-489a-b62a-466840a67404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params for New Visitors: {'n_estimators': 250, 'learning_rate': 0.15, 'max_depth': 5}\n",
      "Best F1-Score for New Visitors: 0.8450704225352113\n",
      "\n",
      "Gradient Boosting - New Visitors Results\n",
      "    n_estimators  learning_rate  max_depth  accuracy  precision    recall  \\\n",
      "46           250           0.15          5  0.918819   0.810811  0.882353   \n",
      "74           350           0.10          5  0.918819   0.819444  0.867647   \n",
      "82           400           0.01          5  0.918819   0.828571  0.852941   \n",
      "80           400           0.01          2  0.918819   0.828571  0.852941   \n",
      "66           350           0.01          5  0.918819   0.828571  0.852941   \n",
      "..           ...            ...        ...       ...        ...       ...   \n",
      "19           200           0.01         10  0.885609   0.740260  0.838235   \n",
      "11           150           0.10         10  0.889299   0.763889  0.808824   \n",
      "87           400           0.05         10  0.889299   0.763889  0.808824   \n",
      "15           150           0.15         10  0.889299   0.763889  0.808824   \n",
      "7            150           0.05         10  0.892989   0.791045  0.779412   \n",
      "\n",
      "    f1_score   roc_auc  \n",
      "46  0.845070  0.941394  \n",
      "74  0.842857  0.936033  \n",
      "82  0.840580  0.926615  \n",
      "80  0.840580  0.931686  \n",
      "66  0.840580  0.930346  \n",
      "..       ...       ...  \n",
      "19  0.786207  0.890032  \n",
      "11  0.785714  0.920458  \n",
      "87  0.785714  0.925674  \n",
      "15  0.785714  0.930310  \n",
      "7   0.785185  0.873153  \n",
      "\n",
      "[96 rows x 8 columns]\n",
      "\n",
      "Best Params for Returning Visitors: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 2}\n",
      "Best F1-Score for Returning Visitors: 0.6363636363636364\n",
      "\n",
      "Gradient Boosting - Returning Visitors Results\n",
      "    n_estimators  learning_rate  max_depth  accuracy  precision    recall  \\\n",
      "8            150           0.10          2  0.881517   0.555556  0.744681   \n",
      "65           350           0.01          3  0.877370   0.542169  0.765957   \n",
      "81           400           0.01          3  0.877370   0.542169  0.765957   \n",
      "36           250           0.05          2  0.879739   0.550000  0.748936   \n",
      "20           200           0.05          2  0.878555   0.546296  0.753191   \n",
      "..           ...            ...        ...       ...        ...       ...   \n",
      "71           350           0.05         10  0.883294   0.589623  0.531915   \n",
      "87           400           0.05         10  0.882109   0.584112  0.531915   \n",
      "90           400           0.10          5  0.880924   0.577982  0.536170   \n",
      "95           400           0.15         10  0.881517   0.581395  0.531915   \n",
      "74           350           0.10          5  0.881517   0.582160  0.527660   \n",
      "\n",
      "    f1_score   roc_auc  \n",
      "8   0.636364  0.919956  \n",
      "65  0.634921  0.915418  \n",
      "81  0.634921  0.916103  \n",
      "36  0.634234  0.917726  \n",
      "20  0.633274  0.916304  \n",
      "..       ...       ...  \n",
      "71  0.559284  0.910855  \n",
      "87  0.556793  0.910753  \n",
      "90  0.556291  0.909470  \n",
      "95  0.555556  0.910486  \n",
      "74  0.553571  0.910963  \n",
      "\n",
      "[96 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "'''Step 4 Gradient Boosting'''\n",
    "# tuning hyperparameters\n",
    "\n",
    "def tune_gradient_boosting(X_train, y_train, X_val, y_val, label=''):\n",
    "    param_grid = {\n",
    "        'max_depth': [2, 3, 5, 10],\n",
    "        'n_estimators': [150,200,250,300, 350, 400], \n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.15]} \n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "\n",
    "    for n in param_grid['n_estimators']:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for depth in param_grid['max_depth']:\n",
    "                model = GradientBoostingClassifier(\n",
    "                    n_estimators=n,\n",
    "                    learning_rate=lr,\n",
    "                    max_depth=depth,\n",
    "                    random_state=123)\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "                acc = accuracy_score(y_val, y_pred)\n",
    "                prec = precision_score(y_val, y_pred)\n",
    "                rec = recall_score(y_val, y_pred)\n",
    "                f1 = f1_score(y_val, y_pred)\n",
    "                roc_auc = roc_auc_score(y_val, y_proba)\n",
    "\n",
    "                results.append({\n",
    "                    'n_estimators': n,\n",
    "                    'learning_rate': lr,\n",
    "                    'max_depth': depth,\n",
    "                    'accuracy': acc,\n",
    "                    'precision': prec,\n",
    "                    'recall': rec,\n",
    "                    'f1_score': f1,\n",
    "                    'roc_auc': roc_auc})\n",
    "    \n",
    "                if f1 > best_score:\n",
    "                    best_score = f1\n",
    "                    best_params = {\n",
    "                        'n_estimators': n,\n",
    "                        'learning_rate': lr,\n",
    "                        'max_depth': depth,}\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values(by='f1_score', ascending=False)\n",
    "    print(f\"\\nBest Params for {label}:\", best_params)\n",
    "    print(f\"Best F1-Score for {label}:\", best_score)\n",
    "    print(f\"\\nGradient Boosting - {label} Results\")\n",
    "    print(df_results)\n",
    "\n",
    "    return best_params, df_results\n",
    "\n",
    "gbn_best_param, gbn_results = tune_gradient_boosting(\n",
    "    X_train_new_resampled_unscaled,\n",
    "    y_train_new_resampled_unscaled,\n",
    "    X_val_new,\n",
    "    y_val_new,\n",
    "    label='New Visitors')\n",
    "\n",
    "gbr_best_param, gbr_results = tune_gradient_boosting(\n",
    "    X_train_return_resampled_unscaled,\n",
    "    y_train_return_resampled_unscaled,\n",
    "    X_val_return,\n",
    "    y_val_return,\n",
    "    label='Returning Visitors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a986c7c-ed55-4708-a8c2-b7bde75577ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model      Group  Accuracy  Precision    Recall  F1-score  \\\n",
      "0  Logistic Regression        New  0.911439   0.833333  0.808824  0.820896   \n",
      "1        Decision Tree        New  0.915129   0.826087  0.838235  0.832117   \n",
      "2        Random Forest        New  0.918819   0.828571  0.852941  0.840580   \n",
      "3    Gradient Boosting        New  0.918819   0.810811  0.882353  0.845070   \n",
      "4  Logistic Regression  Returning  0.877370   0.548276  0.676596  0.605714   \n",
      "5        Decision Tree  Returning  0.888033   0.583942  0.680851  0.628684   \n",
      "6        Random Forest  Returning  0.883886   0.563934  0.731915  0.637037   \n",
      "7    Gradient Boosting  Returning  0.881517   0.555556  0.744681  0.636364   \n",
      "\n",
      "    ROC AUC  \n",
      "0  0.923428  \n",
      "1  0.904376  \n",
      "2  0.947117  \n",
      "3  0.941394  \n",
      "4  0.877835  \n",
      "5  0.881122  \n",
      "6  0.921884  \n",
      "7  0.919956  \n"
     ]
    }
   ],
   "source": [
    "results_map = {\n",
    "    ('Logistic Regression', 'New'):   lrn_results,\n",
    "    ('Logistic Regression', 'Returning'): lrr_results,\n",
    "    ('Decision Tree', 'New'):         dtn_results,\n",
    "    ('Decision Tree', 'Returning'):   dtr_results,\n",
    "    ('Random Forest', 'New'):         rfn_results,\n",
    "    ('Random Forest', 'Returning'):   rfr_results,\n",
    "    ('Gradient Boosting', 'New'):     gbn_results,\n",
    "    ('Gradient Boosting', 'Returning'): gbr_results,\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for (model, group), df_res in results_map.items():\n",
    "    best = df_res.sort_values('f1_score', ascending=False).iloc[0]\n",
    "    row = {\n",
    "        'Model': model,\n",
    "        'Group': group,\n",
    "        'Accuracy': best['accuracy'],\n",
    "        'Precision': best['precision'],\n",
    "        'Recall': best['recall'],\n",
    "        'F1-score': best['f1_score'],\n",
    "        'ROC AUC': best['roc_auc']\n",
    "    }\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "df_best = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df_best['Model'] = pd.Categorical(df_best['Model'],\n",
    "    categories=['Logistic Regression','Decision Tree','Random Forest','Gradient Boosting'], ordered=True)\n",
    "df_best['Group'] = pd.Categorical(df_best['Group'], categories=['New','Returning'], ordered=True)\n",
    "\n",
    "df_best = df_best.sort_values(['Group','Model']).reset_index(drop=True)\n",
    "\n",
    "print(df_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f70d11f-9c46-4251-a5d1-b70629a2ec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model      Group  accuracy  precision    recall  f1_score  \\\n",
      "0  LogisticRegression        New  0.902655   0.849315  0.738095  0.789809   \n",
      "1        DecisionTree        New  0.911504   0.846154  0.785714  0.814815   \n",
      "2        RandomForest        New  0.905605   0.842105  0.761905  0.800000   \n",
      "3    GradientBoosting        New  0.896755   0.795181  0.785714  0.790419   \n",
      "4  LogisticRegression  Returning  0.893415   0.692737  0.421769  0.524313   \n",
      "5        DecisionTree  Returning  0.897679   0.659836  0.547619  0.598513   \n",
      "6        RandomForest  Returning  0.906679   0.725581  0.530612  0.612967   \n",
      "7    GradientBoosting  Returning  0.908574   0.702811  0.595238  0.644567   \n",
      "\n",
      "    roc_auc  \n",
      "0  0.902194  \n",
      "1  0.912255  \n",
      "2  0.920121  \n",
      "3  0.913492  \n",
      "4  0.872456  \n",
      "5  0.862289  \n",
      "6  0.935895  \n",
      "7  0.934933  \n"
     ]
    }
   ],
   "source": [
    "data_map = {\n",
    "    'New': {\n",
    "        'X_train': X_train_newf, 'y_train': y_train_newf,\n",
    "        'X_test':  X_test_new,  'y_test':  y_test_new,\n",
    "    },\n",
    "    'Returning': {\n",
    "        'X_train': X_train_returnf, 'y_train': y_train_returnf,\n",
    "        'X_test':  X_test_return,  'y_test':  y_test_return,\n",
    "    }\n",
    "}\n",
    "\n",
    "best_params_map = {\n",
    "    'LogisticRegression': {\n",
    "        'New':   lrn_best_param,\n",
    "        'Returning': lrr_best_param,\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'New':   dtn_best_param,\n",
    "        'Returning': dtr_best_param,\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'New':   rfn_best_param,\n",
    "        'Returning': rfr_best_param,\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'New':   gbn_best_param,\n",
    "        'Returning': gbr_best_param,\n",
    "    }\n",
    "}\n",
    "\n",
    "model_map = {\n",
    "    'LogisticRegression': LogisticRegression,\n",
    "    'DecisionTree':        DecisionTreeClassifier,\n",
    "    'RandomForest':        RandomForestClassifier,\n",
    "    'GradientBoosting':    GradientBoostingClassifier,\n",
    "}\n",
    "\n",
    "def make_pipeline(model_name, ModelClass, params):\n",
    "    steps = []\n",
    "    if model_name == 'LogisticRegression':\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "    steps.append(('clf', ModelClass(**params, random_state=123)))\n",
    "    return Pipeline(steps)\n",
    "    \n",
    "def eval_on_test(model_name, X_train, y_train, X_test, y_test, ModelClass, params, threshold=0.5):\n",
    "    if model_name == 'LogisticRegression':\n",
    "        params['max_iter'] = 30000\n",
    "    pipe = make_pipeline(model_name, ModelClass, params)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_proba > threshold).astype(int)\n",
    "    return {\n",
    "        'accuracy':   accuracy_score(y_test, y_pred),\n",
    "        'precision':  precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall':     recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1_score':   f1_score(y_test, y_pred, zero_division=0),\n",
    "        'roc_auc':    roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "\n",
    "   \n",
    "records = []\n",
    "for model_name, ModelClass in model_map.items():\n",
    "    for group, data in data_map.items():\n",
    "        params = best_params_map[model_name][group]\n",
    "        if model_name == 'LogisticRegression':\n",
    "            metrics = eval_on_test(\n",
    "                model_name, data['X_train'], data['y_train'],\n",
    "                data['X_test'],  data['y_test'],\n",
    "                ModelClass, params,\n",
    "                threshold=0.3)\n",
    "        else:\n",
    "            metrics = eval_on_test(\n",
    "                model_name,\n",
    "                data['X_train'], data['y_train'],\n",
    "                data['X_test'],  data['y_test'],\n",
    "                ModelClass, params)\n",
    "        # 把參數內容也攤平放進 row 裡\n",
    "        row = {'Model': model_name, 'Group': group}\n",
    "        row.update(metrics)\n",
    "        # row.update(params)\n",
    "        records.append(row)\n",
    "\n",
    "df_eval = pd.DataFrame(records)\n",
    "df_eval['Model'] = pd.Categorical(df_eval['Model'], \n",
    "    categories=['LogisticRegression','DecisionTree','RandomForest','GradientBoosting'], ordered=True)\n",
    "df_eval['Group'] = pd.Categorical(df_eval['Group'], categories=['New','Returning'], ordered=True)\n",
    "df_eval = df_eval.sort_values(['Group','Model']).reset_index(drop=True)\n",
    "\n",
    "print(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da31eba-a33c-45e1-8b48-6ba13aad4635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1ebcb-d3d3-4712-93e7-b673f133af17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
